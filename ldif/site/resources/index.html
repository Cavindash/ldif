<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>LDIF - Linked Data Integration Framework</title>

  
  <style>
body { background: white; color: black; font-family: sans-serif; line-height: 1.4em; padding: 2.5em 3em; margin: 0; }
:link { color: #00c; }
:visited { color: #609; }
a:link img { border: none; }
a:visited img { border: none; }
h1, h2, h3 { background: white; color: #800; }
h1 { font: 170% sans-serif; margin: 0; }
h2 { clear: both; font: 140% sans-serif; margin: 2.5em 0 -0.5em 0; }
h3 { font: 120% sans-serif; margin: 1.5em 0 -0.5em 0; }
h4 { font: bold 100% sans-serif; }
h5 { font: italic 100% sans-serif; }
h6 { font: small-caps 100% sans-serif; }
.hide { display: none; }
pre { background: #fff6bb; font-family: monospace; line-height: 1.2em; padding: 1em 2em; }
dt { font-weight: bold; margin-top: 0; margin-bottom: 0; }
dd { margin-top: 0; margin-bottom: 0; }
code, tt { font-family: monospace; }
ul.toc { list-style-type: none; }
ol.toc li a { text-decoration: none; }
.note { color: red; }
#header { border-bottom: 1px solid #ccc; }
#logo { float: right; }
#authors { clear: right; float: right; font-size: 80%; text-align: right; }
#content { clear: both; margin: 2em auto 0 0; text-align: justify }
#download, #demo { float: left; font-family: sans-serif; margin: 1em 0 1.5em; text-align: center; width: 50%; }
#download h2, #demo h2 { font-size: 125%; margin: 1.5em 0 -0.2em 0; }
#download small, #demo small { color: #888; font-size: 80%; }
#footer { border-top: 1px solid #ccc; color: #aaa; margin: 2em 0 0; }
@media Print {
* { font-size: 92%; }
body { padding: 0; line-height: 1.2em; }
#content { margin: 0; width: 100%; }
}
@media Aural {
h1 { stress: 20; richness: 90; }
h2 { stress: 20; richness: 90; }
h3 { stress: 20; richness: 90; }
.hide { speak: none; }
dt { pause-before: 20%; }
pre { speak-punctuation: code; }
}
.Stil1 {color: #FF0000}
table {border-collapse: collapse; margin-bottom: .75em; border-top: 1px solid #D4DCE8; border-bottom: 1px solid #D4DCE8}
table th {text-align: left; background: #EAF3FA; padding: .5em .9em; height: 21px; font-weight: bold; font-size: 1em; color: #333}
table th a {text-decoration: none !important}
table td {vertical-align: top; text-align: left; padding: .5em 1em; border-bottom: 1px solid #eee}
table tr.even {background-color: #fff}
table tr.odd {background-color: #f6f6f6}
  </style>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body>
<div id="logo" align="right"><a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/index.html"><img src="img/fu-logo.gif" alt="Freie UniversitÃ¤t Berlin Logo" border="0"></a>&nbsp;<br>
<a href="http://mediaeventservices.com"><img src="img/logo.gif" alt="Media Event Services"></a></div>

<div id="header">
<h1 style="font-size: 250%;">LDIF - Linked Data Integration Framework </h1>
</div>

<div id="tagline">A component for building Linked Data applications
</div>

<div id="authors"><a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/team/SchultzAndreas.html">Andreas
Schultz</a><br>
<a href="mailto:andrea.matteini@gmail.com">Andrea Matteini</a><br>
<a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/team/IseleRobert.html">Robert
Isele</a><br>
<a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/team/BizerChristian.html">Chris
Bizer</a><br>
<a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/team/BeckerChristian.html">Christian
Becker</a><br>
</div>

<div id="content">
<p>The LDIF - Framework translates heterogeneous Linked Data from the
Web into <br>
a clean, local target representation while keeping track of data
provenance.</p>
<div id="download">
<h2><a href="http://www4.wiwiss.fu-berlin.de/bizer/ldif/releases/ldif-0.3.zip">Download
LDIF</a><br>
</h2>
<small>v0.3, released 2011-09-29</small>
</div>
<h2 id="news">News</h2>
<ul>
  <li><strong>9/29/2011: Version 0.3 released.</strong> The third
LDIF release provides access modules for replicating datasets locally
via file download, crawling or SPARQL. It offers a URI minting feature
to produce descriptive URIs and a scheduler to launch data import and
integration jobs. Configuration files are validated via XML Schema before continuing execution.<br>
</li>
  <li><strong>8/25/2011: Version 0.2 released.</strong> The second
LDIF release provides improved performance (faster data loading,
parallelization of the data translation), smaller memory footprint, a
new N-Triples output module, new performance evaluation results for use
cases up to 100 million triples.<br>
  </li>

  <li><strong>6/29/2011: Version 0.1 released.</strong> This alpha
version provides for translating data that is represented using
different source vocabularies into a single target vocabulary and for
replacing different URIs that refer to the same real-world entity with
a single target URI.</li>
</ul>
<h2 id="contents">Contents</h2>
<ol class="toc">
  <li><a href="#about">About LDIF</a></li>
  <li><a href="#components">LDIF components</a></li>
  <li><a href="#quickstart">Quick start</a></li>
  <li><a href="#example1">Integration example: </a><a href="#example1">Using LDIF for integrating Life
Science Data</a><a href="#example1">
    </a></li>

  <li><a href="#example2">Integration example: </a><a href="#example2">Using LDIF for integrating Data from the Music Domain</a><a href="#example2"></a></li>
  <li><a href="#performance">Performance Evaluation</a> </li>
  <li><a href="#configuration">Configuration options</a></li>
  <li><a href="#development">Source code and development</a></li>
  <li><a href="#history">Version history</a></li>
  <li><a href="#feedback">Support and Feedback</a></li>
  <li><a href="#references">References</a></li>
</ol>
<h2 id="about">1. About LDIF<br>
</h2>
<p>The <a href="http://linkeddatabook.com/editions/1.0/index.html#htoc23">Web of
Linked Data</a>
grows rapidly and contains data from a wide range of different domains,
including life science data, geographic data, government data, library
and media data, as well as cross-domain datasets such as DBpedia or
Freebase. <a href="http://linkeddatabook.com/editions/1.0/index.html#htoc75">Linked
Data applications</a> that want to consume data from this global data
space face the challenges that:</p>
<ol>
  <li> data sources use a wide range of different RDF vocabularies to
represent data about the same type of entity. </li>
  <li>the
same real-world entity, for instance a person or a place, is
identified with different URIs within different data sources. </li>
</ol>
<p>This usage of different vocabularies as well as the usage of URI
aliases makes it very cumbersome for an application
developer to write <a href="http://www.w3.org/TR/rdf-sparql-query/">SPARQL</a>
queries against Web data which originates from multiple sources. In
order to ease using Web data in the application context, it is thus
advisable to translate data to a single target vocabulary (<a href="http://linkeddatabook.com/editions/1.0/index.html#htoc86">vocabulary
mapping</a>) and to replace URI aliases with a single target URI on the
client
side (<a href="http://linkeddatabook.com/editions/1.0/index.html#htoc87">identity
resolution</a>), before starting to ask SPARQL queries against the
data. </p>
<p>Up-till-now, there have not been any integrated tools that help
application developers with these tasks. With LDIF, we try to fill this
gap and provide an an open-source Linked Data Integration Framework
that can be used by Linked Data applications to translate Web data and
normalize URI while keeping track of data provenance.<br>
<br>
LDIF provides an expressive mapping language for translating data from
the various vocabularies that are used on the Web into a consistent,
local target vocabulary. LDIF includes an identity resolution component
which discovers URI aliases in the input data and replaces them with a
single target URI based on user-provided matching heuristics. For
provenance tracking, the LDIF framework employs the Named Graphs data
model.<br>
</p>
<p>The figure below shows the schematic <a href="http://linkeddatabook.com/editions/1.0/index.html#htoc84">architecture
of
Linked Data applications</a>
that
implement the crawling/data warehousing pattern. The figure highlights
the steps of the data integration process that are currently
supported by LDIF.<br>
<br>
<span class="Stil1"> </span> </p>
<p style="text-align: left; margin-left: 120px;"><img style="border: 1px solid ; width: 650px; height: 496px;" alt="Example-architecture of an integration aware Linked Data application" src="img/linkeddataapp.png"></p>
<p style="text-align: left; margin-left: 120px;">&nbsp;</p>
<h2 id="components">2. LDIF Components<br>
</h2>
<p>The LDIF Framework consists of the Runtime Environment and a set of
pluggable modules. The pluggable modules are organized as data access
components, a data transformation components and a data output
components.
</p>
<p><img alt="LDIF components" src="img/LDIFcomponents.png" height="92" width="299"></p>
Currently, we have implemented the following modules: <br>
<h3>Web Data Access: Triple/Quad Dump Import</h3><br>

In order to get a local replication of datasets from the Web of Data
the simplest way is to download a file containing the data set. The
triple/quad dump import does exactly this, with the difference that
LDIF generates a provenance graph for a triple dump import, whereas it
takes the given graphs from a quad dump import as provenance graphs.<br>
<h3>Web Data Access: Crawler Import</h3>
<br>
Datasets that can only be accessed via dereferencable URIs are a good candidate for a crawler. In LDIF we thus integrated <a href="http://code.google.com/p/ldspider/">LDSpider</a> for crawl import jobs. The configuration files for crawl import jobs are specified in the <a href="index.html#config">configuration</a> section. Each crawled URI is put into a seperate named graph for provenance tracking.<br>
<h3>Web Data Access: SPARQL Import<br>
</h3>
<br>Data sources that can be accessed via SPARQL are replicated by
LDIF's
SPARQL access module. The relevant data to be queried can be further
specified in the configuration file for a SPARQL import job. Data from
each SPARQL import job gets tracked by its own named graph.<br>
<h3>Data Access: N-Quads Loader<br>
</h3>
<p>The current version of LDIF expects input data to be represented as
Named
Graphs and be stored in <a href="http://sw.deri.org/2008/07/n-quads/">N-Quads
format</a> accessible by a URL. The graph URI is used for
provenance tracking. Provenance meta-information describing the graphs
can be provided within a specific provenance graph. The name of
this provenance graph can be set in the configuration file. LDIF does
not make any assumptions about the provenance vocabulary that is used
to describe the graphs, meaning that you can use your provenance
vocabulary of choice. Currently, the provenance information is just
copied to the final output. In future releases, we will use the
provenance information for data quality assessment and data fusion (see
<a href="#nextSteps">Next Steps</a>).<br>
</p>
<h3>Transformation: R2R Data Translation
</h3>
<p>LDIF employs the <a href="http://www4.wiwiss.fu-berlin.de/bizer/r2r/">R2R Framework</a>
to translate Web data that is represented using terms from different
vocabularies into a single target vocabulary. Vocabulary mappings are
expressed using the <a href="http://www4.wiwiss.fu-berlin.de/bizer/r2r/spec/">R2R Mapping
Language</a>.
The language provides for simple transformations as well as for more
complex structural transformations and property value transformations
such as normalizing different units of measurement or complex string
manipulations. The syntax of the
R2R Mapping Language is very similar to the query language SPARQL,
which eases the learning curve. The expressivity of the language
enabled us to deal with all requirements that we have encountered so
far when translatingLinked Data from the Web into a target
representation (evaluation in [2]). </p>
<p>An overview and examples for mappings are given on the <a href="http://www4.wiwiss.fu-berlin.de/bizer/r2r/">R2R website</a>.<br>
The <a href="http://www4.wiwiss.fu-berlin.de/bizer/r2r/spec/">specification
and user manual</a> is provided as a separate document.<br>
</p>
<h3>Transformation: Silk Identity Resolution </h3>
<p>
LDIF employs the <a href="http://www4.wiwiss.fu-berlin.de/bizer/silk/">Silk
Link Discovery Framework</a>
to find different URIs that are used within different data sources to
identify the same real-world entity. For each set of duplicates which
have been identified by Silk, LDIF replaces all URI aliases with a
single target URI within the output data. In addition, it adds
owl:sameAs links pointing at the original URIs, which makes it possible
for applications to refer back to the data sources on the Web. If the
LDIF input data already contains owl:sameAs links, the referenced
URIsare normalized accordingly (optional, see <a href="#configuration">configuration</a>).
Silk is a
flexible identity
resolution framework that allows the user to specify identity
resolution heuristics which combine different types of matchers using
the declarative <a href="http://www4.wiwiss.fu-berlin.de/bizer/silk/spec/">Silk - Link
Specification Language</a>.</p>
<p>An overview and examples can be found on the <a href="http://www4.wiwiss.fu-berlin.de/bizer/silk/">Silk website</a>.<br>
</p>
<h3 style="text-align: left;">Data Output: N-Quads Writer<br>
</h3>
<p>The N-Quads writer dumps the final output of the integration
workflow into a single N-Quads file. This file contains the translated
versions of all graphs from the input graph set as well as the content
of the provenance graph and sameAs-links. <br>
</p>
<h3 style="text-align: left;">Data Output: N-Triples Writer<br>
</h3>
<p>The N-Triples writer dumps the final output of the integration
workflow into a single N-Triples file.<br>
</p>
<h3 style="text-align: left;">Runtime Environment </h3>
<p style="text-align: left;"> The Runtime Environment manages the data
flow between the various stages and the caching of the intermediate
results. In order to parallelize the data processing, the data is
partitioned into entities prior to supplying it to a transformation
module. An entity represents a Web resource together with all data that
is required by a transformation module to process this resource.
Entities consist of one or more graph paths and include a provenance
URI for each node. Each transformation module specifies which paths
should be included into the entities it processes. Splitting the work
into fine-granular entities, allows LDIF to parallelize the work into
multiple threats and will, in the next release, allow thework to be
parallelized on multiple machines using Hadoop. For the LDIF V0.3
release, we have implemented only an
in-memory version of the runtime environment. This implementation keeps
all intermediate results in
memory. It is very fast but scales only depending on the amount of
memory
available. For instance, integrating 25 million triples required 5 GB
memory within one of our <a href="benchmark.html">experiments</a>. </p>
<h3 id="nextSteps">Next steps for LDIF</h3>
<p>Over the next months, we plan to extend LDIF along the
following lines:<br>
</p>
<ol>
 <li>Implement a <strong>Hadoop Version of the Runtime Environment </strong>in
order to be able to scale to really large amounts of input data.
Processes and data will be distributed over a cluster of machines.</li>
  <li>Add a <strong>Data Quality Evaluation and Data Fusion Module</strong>
which allows Web data to be filtered according to different data
quality assessment policies and provides for fusing Web data according
to different conflict resolution methods.</li>
  <li><strong>Flexible integration workflow</strong>. Currently the
integration flow is static and can only be influenced by predefined
configuration parameters. We plan to make the workflow and its
configuration more flexible in order to make it easier to include
additional modules that cover other data integration aspects. </li>
</ol>
<h2 id="quickstart">3. Quick start</h2>
<br>
In order to experiment with LDIF follow these steps:<br>
<ol>
  <li>Download the latest release (download link at the top of this
page)</li>
  <li>Unpack the archive to an arbitrary location</li>
  <li>To run the command line tool type the following in the root
directory of LDIF:<br>
  </li>
</ol>
<pre>bin/ldif &lt;path-to-scheduler-config-file&gt;<br><br>bin\ldif.bat &lt;path-to-scheduler-config-file&gt;<br></pre>
For details about the configuration file and parameters, see <a href="#config">Section 6</a>. A detailed example is given next.<br>
<h2 id="examples"><a name="example1"></a>4. Example: Using LDIF to
integrate Life Science Data </h2>

<br>

<p>This example shows how LDIF is applied to integrate data originating
from five Life Science sources. </p>

<p>The example is taken from a joined project with <a href="http://www.vulcan.com/TemplateHome.aspx?contentId=1">Vulcan Inc</a>.
and <a href="http://www.ontoprise.de/">ontoprise GmbH</a> about
extending <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/BeckerBizerErdmannGreaves-SMW-LDE-Poster-ISWC2010.pdf">Semantic
Media Wiki+ with a Linked Data Integration Framework</a>. <br>
<br>
In this example, the following data sources are translated into a
common <a href="resources/Wiki.owl">Wiki ontology</a>:</p>

<ul>
<li><a href="http://mouse.brain-map.org/">Allen Mouse Brain Atlas</a>
is a growing collection of online public resources integrating
extensive gene expression and neuroanatomical data. </li><li><a href="http://www.genome.jp/kegg/genes.html">KEGG GENES</a>, a
collection of gene catalogs for all complete genomes generated from
publicly available resources, mostly NCBI RefSeq</li><li><a href="http://www.genome.jp/kegg/pathway.html">KEGG Pathway</a>,
a collection of pathway maps representing knowledge on the molecular
interaction and reaction networks</li><li><a href="http://pharmgkb.org/">PharmGKB</a>, which provides data
on gene information, disease and drug pathways, and SNP variants</li><li><a href="http://www.uniprot.org/">Uniprot</a>, which provides
information on protein sequence and function </li>
</ul>

<p>The following mapping file provides for translating the vocabularies
used by the source datasets into the Wiki ontology. </p>

<ul>
<li>R2R mapping file: <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/life-science/mappings/ALL-to-Wiki.r2r.ttl">ALL-toWiki.r2r.ttl</a>
    <span style="text-decoration: underline;"></span></li>
</ul>

<p>The following Silk identity resolution heuristics are used to find
genes and other expressions that are described in multiple datasets.</p>
<ul>
  <li><a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/life-science/linkSpecs/genes.xml">genes.xml</a>,</li>
  <li><a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/life-science/linkSpecs/diseases.xml">diseases.xml</a>,</li>
  <li><a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/life-science/linkSpecs/pathways.xml">pathways.xml</a></li>
</ul>
<p>To run the example, please download LDIF and use the following LDIF
configuration. The configuration options are explained in the Section <a href="#configuration">Configuration</a> below. <br>
</p>
<ul>
  <pre>&lt;integrationJob&gt;<br>  &lt;properties&gt;life-science.properties&lt;/properties&gt;<br>  &lt;sources&gt;sources&lt;/sources&gt;<br>  &lt;linkSpecifications&gt;linkSpecs&lt;/linkSpecifications&gt;<br>  &lt;mappings&gt;mappings&lt;/mappings&gt;<br>  &lt;output&gt;output.nq&lt;/output&gt;<br>&lt;/integrationJob&gt;</pre>
</ul>
<p>Execution instructions:</p>

<ul>
<li>Change into the LDIF root dir.</li><li>Under Unix type:</li><ul><pre>bin/ldif-integrate examples/life-science/integration-config.xml</pre></ul><li>Under Windows type:</li><ul><pre>bin\ldif-integrate.bat examples\life-science\integration-config.xml</pre></ul>
</ul>

<h3>Example Data Translation </h3>

<p>In the following, we explain the data translation that is performed
for the example of one entity that is described in two input datasets:</p>

<ul>
<li><strong>Example <span style="font-style: italic;">input</span> </strong>(reduced
to two source datasets, represented using the <a href="http://www4.wiwiss.fu-berlin.de/bizer/TriG/">TriG Syntax</a>):<br>
  </li>
</ul>

<pre style="margin-left: 40px;">01:  @prefix aba-voc: &lt;http://brain-map.org/gene/0.1#&gt; .<br>02:  @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .<br>03:  @prefix uniprot: &lt;http://purl.uniprot.org/core/&gt; . <br>04: &nbsp;<br>05:  &lt;file:///aba_mouse_20101010_1000.nq&gt; {<br>06:    &lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt; aba-voc:entrezgeneid "<span style="font-weight: bold;">18387</span>" ;<br>07:      aba-voc:gene-aliases _:Ab12290 .<br>08:   _:Ab12290 &lt;http://brain-map.org/gene/0.1#aliassymbol&gt; "<span style="font-weight: bold;">Oprk1</span>" .<br>09:  }<br>10:<br>11:  &lt;file:///datasets/uniprot-organism-human-reviewed-complete_1000.nq&gt; {<br>12:    &lt;http://purl.uniprot.org/uniprot/P61981&gt; rdfs:seeAlso &lt;http://purl.uniprot.org/geneid/<span style="font-weight: bold;">18387</span>&gt; .<br>13:    &lt;http://purl.uniprot.org/geneid/<span style="font-weight: bold;">18387</span>&gt; uniprot:database "GeneID" .<br>14:    &lt;http://purl.uniprot.org/uniprot/P61981&gt; uniprot:encodedBy &lt;file:///storage/datasets/uniprot-organism-human-reviewed-complete.rdf#_503237333438003B&gt; .<br>15:  }<br></pre>

<ul>
<li><strong>Example <span style="font-style: italic;">output</span> :</strong><br>
  </li>
</ul>

<pre style="margin-left: 40px;">01: @prefix smwprop: &lt;http://mywiki/resource/property/&gt; .<br>02: @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .<br>03: <br>04: &lt;file:///aba_mouse_20101010_1000.nq&gt; {<br>05:  &nbsp;&lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt; smwprop:EntrezGeneId "<span style="font-weight: bold;">18387</span>"^^xsd:int .<br>06:  &nbsp;&lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt; smwprop:GeneSymbol "<span style="font-weight: bold;">Oprk1</span>"^^xsd:string .<br>07: }<br>08: <br>09: &lt;file:///datasets/uniprot-organism-human-reviewed-complete_1000.nq&gt; {<br>10:   &lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt; smwprop:EntrezGeneId "<span style="font-weight: bold;">18387</span>"^^xsd:int .<br>11:   &lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt; <span style="font-family: monospace;">owl:sameAs </span><span style="font-family: monospace;">&lt;file:///storage/datasets/uniprot-organism-human-reviewed-complete.rdf#_503237333438003B&gt; .</span><br>12: }<br></pre>

<p>The example input and output needs some explanation:</p>

<ul>
<li>There are two source graphs, each containing data from a
different source: ABA (input: line 5 to 9) and Uniprot (input: line 11
to 15).</li>
</ul>

<p>Identity resolution: </p>

<ul>
<li>Both graphs contain data about the same entity:</li><ul><li>In the ABA dataset the entity is identified using the URI <span style="font-family: monospace;">&lt;http://brain-map.org/mouse/brain/Oprk1.xml&gt;
      </span>(input: line 6)</li><li>In the Uniprot dataset the entity is identified using the URI <span style="font-family: monospace;"></span><br>
      <span style="font-family: monospace;">&lt;file:///storage/datasets/uniprot-organism-human-reviewed-complete.rdf#_503237333438003B&gt;</span>
(input: line 14)</li></ul><li>Since the Silk identity resolution heuristic concludes that both
URIs identify the same entity, the both URIs are replaced in the output
with a single URI (in this case the ABA one, output: lines 5, 6 and 10).</li><li>The rewritten URI is linked by <span style="font-family: monospace;">owl:sameAs</span> to the original URI
(output: line 11).<br>
  </li>
</ul>

<p>Data Translation: </p>

<ul>
<li>In the target vocabulary Entrez Gene IDs should be
represented using the smwprop:EntrezGeneId property. Property values
should be represented as xsd:Integers. </li><li>Thus, the
aba-voc:entrezgeneid triple in the first graph (line 6) is translated
into a smwprop:EntrezGeneId triple in the output data (line 5) and a
datatype URI is added to the literal. </li><li>The
smwprop:GeneSymbol triple in line 6 of the output is generated by a
structural transformation out of the two triples in lines 7 and 8 of
the input data. </li><li>In the Uniprot case the <span style="font-family: monospace;">smwprop:EntrezGeneId</span>
value was extracted from the URI string <span style="font-family: monospace;">&lt;http://purl.uniprot.org/geneid/18387&gt;
    </span><span style="font-family: sans-serif;">(input: line 12)</span><span style="font-family: monospace;"><span style="font-family: sans-serif;">.
    </span></span></li><li>The quad with the property <span style="font-family: monospace;">smwprop:EntrezGeneId</span>
on line 10 in the output was produced by a complex mapping that had to
consider all three quads of the input (lines 12-14).</li>
</ul>
<h2 id="examples"><a name="example2"></a>5. Example: Using LDIF for integrating Data from the Music Domain</h2>

<br><br>
This example shows how LDIF is applied to integrate data originating
from the following sources containing data from the music domain:<br>
<ul>
  <li><a href="http://dbpedia.org/">DBpedia</a></li>
  <li><a href="http://rdf.freebase.com/">Freebase</a></li>
  <li><a href="http://musicbrainz.dataincubator.org/">MusicBrainz</a> (at Talis)<br>
  </li>
  <li><a href="http://www.bbc.co.uk/music/artist/">BBC Music</a></li>
</ul>Each source is accessed via the appropriate access module. The
DBpedia
dataset is downloaded, Freebase is crawled because of lack of other
access possibilities, MusicBrainz and BBC Music are both accessed via
SPARQL because no download of the dataset is available and crawling is
in general inferior, because you might not gather all the instances you
are interested in.<br>
<br>
The following configuration files are used for importing the different sources<br>
<ul>
  <li>DBpedia
      <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/DBpediaDumpImport.xml">Dump</a>
  </li>
  <li>Freebase
      <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/FreebaseCrawlImport.xml">Crawl</a>
  </li>
  <li>MusicBrainz
      <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/MusicBrainzSPARQLImport_MusicArtist.xml">Sparql-MusicArtist</a>
      , <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/MusicBrainzSPARQLImport_Label.xml">Sparql-Label</a>, <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/MusicBrainzSPARQLImport_Record.xml">Sparql-Record</a>
  </li>
  <li>BBC Music
      <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/BBCSPARQLImportMusicArtist.xml">Sparql-MusicArtist</a> , <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/BBCSPARQLImportRecord.xml">Sparql-Record</a>, <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/BBCSPARQLImportBirth.xml">Sparql-Birth</a>, <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/ImportJob/BBCSPARQLImportDeath.xml">Sparql-Death</a></li>

</ul>The following mapping file provides for translating the source datasets into our target vocabulary.<br>
<ul>
  <li>R2R mapping file: <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/examples/music/ldif-config/mappings/mappings.ttl">mappings.ttl</a><br>
  </li>
</ul>
The target vocabulary is a mix of existing ontologies like FOAF, Music Ontology, Dublin Core, DBpedia etc.<br>
<br>
The following Silk identity resolution heuristics are used to find
genes and other expressions that are described in multiple datasets:<br>
<ul>
  <li><br>
  </li>
</ul>
<br>
In order to run the example, please download LDIF and run the following commands:<br>
<ul>
<li>Change into the LDIF root dir.</li><li>Under Unix type:</li><ul><pre>bin/ldif examples/music/ldif-config/schedulerConfig.xml</pre></ul><li>Under Windows type:</li><ul><pre>bin\ldif.bat examples\music\ldif-config\schedulerConfig.xml</pre></ul>
</ul>Please note that the execution of the import jobs can take several days, mainly because of the crawl job.<br>
<h2 id="performance"><a name="performance"></a>6.
Performance Evaluation</h2>
<p>We regularly carry out performance evaluations. For more
details and the latest results please visit our <a href="benchmark.html">Benchmark results</a> page.</p>


<h2 id="configuration">7. Configuration
Options</h2>
<p>This section describes how an LDIF configuration files look like and
which parameters you can modify to change the runtime behavior of LDIF.</p>

<h3>Schedule Job Configuration</h3>
<p>A Schedule Job updates the local sources and it is configured with an XML document, whose structure is described by this <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/ldif-core/src/main/resources/xsd/SchedulerConfig.xsd">XML Schema</a>.</p>

<p>A typical configuration document looks like this:</p>
<pre>
&lt;scheduler&gt;
    &lt;properties&gt;scheduler.properties&lt;/properties&gt;
    &lt;dataSources&gt;datasources&lt;/dataSources&gt;
    &lt;importJobs&gt;importJobs&lt;/importJobs&gt;
    &lt;integrationJobs&gt;integration-config.xml&lt;/integrationJob&gt;
    &lt;dumpLocation&gt;dumps&lt;/dumpLocation&gt;
&lt;/scheduler&gt;
</pre>

<p>It has the following elements:</p>
<ul>
  <li><i>properties</i> - the path to a Java properties file for configuration parameters, see below for more details; </li>
  <li><i>dataSources</i> - a directory containing the Data Sources configurations;</li>
  <li><i>importJobs</i> - a directory containing the Import Jobs configurations;</li>
  <li><i>integrationJob</i> - a document containing the Integration Job configurations;</li>
  <li><i>dumpLocation</i> - a directory where the local dumps should be loaded.</li>
</ul>
<p>All the paths are relative to the config file location.</p>

<h4>Configuration Properties</h4>
<p>In the Schedule Job configuration file you can specify a (Java) properties file to further tweak certain parameters concerning the workflow.
    Here is a list with all properties that can be set at the moment and the possible values for each property:</p>
<ul>
  <li><p><i>provenanceGraphURI</i> <br/> Specify the graph containing the provenance information. As of LDIF V0.3, Quads from this graph are only written to the final output dataset, but are
      processed any further in the integration workflow. Default graph name: <span style="font-family: monospace;">http://www4.wiwiss.fu-berlin.de/ldif/provenance</span></p>
  <pre>provenanceGraphURI = http://www4.wiwiss.fu-berlin.de/ldif/provenance</pre></li>
</ul>

<ul>
  <li><p><i>oneTimeExecution</i> <br/>If _true_, the Scheduler executes all the Jobs at most once. Import Jobs are evaluated first and then (as all of these are finished)
      the Integration Job starts. Default: <span style="font-family: monospace;">false</span></p>
  <pre>oneTimeExecution = true | false</pre></li>
</ul>


<h3>Integration Job Configuration</h3>
<p>An Integration Job is configured with an XML document, whose structure is described by this <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/ldif-core/src/main/resources/xsd/IntegrationJob.xsd">XML Schema</a>.
<br/>The current structure is very simple because the integration flow is static at the moment - something that will change in future releases.</p>

<p>A typical configuration document looks like this:</p>
<pre>
&lt;integrationJob&gt;
    &lt;properties&gt;test.properties&lt;/properties&gt;
    &lt;sources&gt;sources&lt;/sources&gt;
    &lt;linkSpecifications&gt;linkSpecs&lt;/linkSpecifications&gt;
    &lt;mappings&gt;mappings&lt;/mappings&gt;
    &lt;output&gt;output.nq&lt;/output&gt;
    &lt;runSchedule&gt;daily&lt;/runSchedule&gt;
&lt;/integrationJob&gt;
</pre>

<p>It has the following elements:</p>
<ul>
  <li><i>properties</i> - the path to a Java properties file for configuration parameters, see below for more details; </li>
  <li><i>sources</i> - a directory containing the source data sets, these files in this directory must be in <a href="http://sw.deri.org/2008/07/n-quads/">N-Quads</a> format
      and may be compressed (.gz, .zip or .bz2);</li>
  <li><i>linkSpecifications</i> - a directory containing the <a href="http://www.assembla.com/wiki/show/silk/Link_Specification_Language">Silk link specifications</a>;</li>
  <li><i>mappings</i> - a directory containing the <a href="http://www4.wiwiss.fu-berlin.de/bizer/r2r/spec/index.html#mappings">R2R mappings</a>;</li>
  <li><i>output</i> - the name of the file to which the output should be written;</li>
  <li><i>runSchedule</i> - how often the integration is expected to be run.</li>
</ul>

<p> All the paths are relative to the config file location.
In this case there is a root directory with the config file and the test.properties file in it.
Furthermore the following directories would be nested in the root directory: linkSpecs, sources and mappings.
Datasets have to be in a local directory.</p>

<h4><a name="configurationproperties"></a>Configuration Properties</h4>
<p>In the Integration Job configuration file you can specify a (Java) properties file to further tweak certain parameters concerning the integration workflow.
   Here is a list with all properties that can be set at the moment and the possible values for each property:</p>

<ul>
  <li><p><i>output</i><br/>
    Specify if all input quads from the input should be included in the output file or only the quads that were mapped/translated by LDIF.
    Default: <span style="font-family: monospace;">mapped-only</span></p>
  <pre>output = all | mapped-only</pre></li>
</ul>

<ul>
  <li><p><i>rewriteURIs</i><br/>
      Specify if URI aliases in the input data should be rewritten to a single target URI in the output data.
      Default: <span style="font-family: monospace;">true</span></p>
  <pre>rewriteURIs = true | false</pre></li>
</ul>

<ul>
  <li><p><i>provenanceGraphURI</i> <br/>
      Specify the graph containing the provenance information. As of LDIF V0.3, Quads from this graph are only written to the final output dataset, but are
      processed any further in the integration workflow. Default graph name: <span style="font-family: monospace;">http://www4.wiwiss.fu-berlin.de/ldif/provenance</span></p>
  <pre>provenanceGraphURI = http://www4.wiwiss.fu-berlin.de/ldif/provenance</pre></li>
</ul>
<ul>
  <li><p><i>validateSources</i><br/>
      Source datasets, R2R mappings and Silk link specifications are all validated before starting with the actual integration.
      Since the syntax validation of the sources (N-Triples / N-Quads files) takes some time (about 15s/GB),
      if you already know that they are correct, it is possible to disable this step by setting the property to false.
      Default:  <span style="font-family: monospace;">true</span></p>
  <pre>validateSources = true | false</pre></li>
</ul>


<ul>
  <li><p><i>useExternalSameAsLinks</i><br/>
  Besides discovering equal entities in the identity resolution phase, LDIF also offers the opportunity to input these relationships in form of owl:sameAs links.
  The NT/N-Quads file with these sameAs-links has to be placed in the source directory with the other datasets.
  If you donât want to use sameAs-links from the input data, set this property to false. Default: <span style="font-family: monospace;">true</span></p>
  <pre>useExternalSameAsLinks = true | false</pre></li>
</ul>

<ul>
  <li><p><i>outputFormat</i><br/>
  Although the default output format is N-Quads, LDIF also offers a triple output as N-Triple. Default: <span style="font-family: monospace;">nq</span></p>
  <pre>outputFormat = nq | nt</pre></li>
</ul>

<ul>
  <li><p><i>uriMinting</i><br/>
  Specify if output resources should be given an URI within the target namespace. URI minting. Default: <span style="font-family: monospace;">false</span></p>
  <pre>uriMinting = true | false</pre></li>
</ul>

<ul>
  <li><p><i>uriMintNamespace</i><br/>
  Specify the namespace into which output resources are translated, if URI minting is enabled. Default: <span style="font-family: monospace;">http://www4.wiwiss.fu-berlin.de/ldif/resource/</span></p>
  <pre>uriMintNamespace = http://www4.wiwiss.fu-berlin.de/ldif/resource/</pre></li>
</ul>

<ul>
  <li><p><i>uriMintLabelPredicate</i><br/>
  Specify the predicates of the resources to translate which are defining the local name of the URI, may be more than one, separated by blank spaces.</p>
  <pre>uriMintLabelPredicate = http://www4.wiwiss.fu-berlin.de/ldif/property/ID http://www4.wiwiss.fu-berlin.de/ldif/property/Label</pre></li>
</ul>

<h3>Import Job Configuration</h3>
<p>An Import Job is configured with an XML document, whose structure is described by this <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/ldif-core/src/main/resources/xsd/ImportJob.xsd">XML Schema</a>.</p>

<p>LDIF supports four different mechanisms to import external data:
    <ul>
    <li>Quad Import Job â import N-Quad dumps</li>
    <li>Triple Import Job â import RDF/N-Triple dumps</li>
    <li>Crawl Import Job â import by dereferencing URIs as RDF data, using the LDspider Web Crawling Framework</li>
    <li>SPARQL Import Job â import by quering a SPARQL endpoint</li>
    </ul>
</p>

<p>A typical config file for a Quad Import Job looks like this: </p>
<pre>
&lt;importJob xmlns="http://www4.wiwiss.fu-berlin.de/bizer/ldif"&gt;
    &lt;internalId&gt;dBpedia.0&lt;/internalId&gt;
    &lt;dataSource&gt;dBpedia&lt;/dataSource&gt;
    &lt;refreshSchedule&gt;daily&lt;/refreshSchedule&gt;
    &lt;quadImportJob&gt;
      &lt;dumpLocation&gt;http://dbpedia.org/dump.nq&lt;/dumpLocation&gt;
    &lt;/quadImportJob&gt;
&lt;/importJob&gt;
</pre>

<h4>Provenance Metadata</h4>
<p>Provenance metadata structure is described by this <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/ldif-core/src/main/resources/owl/provenance.owl">ontology</a>.</p>

<p>For each imported graph, provenance information will contain:
<ul>
    <li>import date and time,</li>
    <li>chosen import type,</li>
    <li>original location (only for Quad and Triple Import Jobs).</li>
</ul>
</p>

<p>A typical provenance graph for a Quad Import Job looks like this:</p>

<pre>
&lt;http://dbpedia.org/graphA&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/ImportedGraph&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
&lt;http://dbpedia.org/graphA&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/hasImportJob&gt; _:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
&lt;http://dbpedia.org/graphB&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/ImportedGraph&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
&lt;http://dbpedia.org/graphB&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/hasImportJob&gt; _:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/ImportJob&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/importId&gt; "dBpedia.0" &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/lastUpdate&gt; "2011-09-21T19:01:00-05:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/hasDatasource&gt; "dBpedia" &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/hasImportType&gt; "quad" &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
_:blanknode &lt;http://www4.wiwiss.fu-berlin.de/ldif/hasOriginalLocation&gt; "http://mes.smw-lde-eu.s3.amazonaws.com/dBpedia_dump.nt.bz2" &lt;http://www4.wiwiss.fu-berlin.de/ldif/provenance&gt; .
</pre>

<h3>Data Source Configuration</h3>
<p>A Data Source is configured with an XML document, whose structure is described by this  <a href="http://www.assembla.com/code/ldif/git/nodes/ldif/ldif-core/src/main/resources/xsd/DataSource.xsd">XML Schema</a>.</p>
<pre>
&lt;dataSource&gt;
    &lt;label&gt;dBpedia&lt;/label&gt;
    &lt;description&gt;DBpedia ist an RDF version of Wikipedia&lt;/description&gt;
   &lt;homepage&gt;http://dbpedia.org&lt;/homepage&gt;
&lt;/dataSource&gt;
</pre>


<h2 id="development">8. Source
Code and Development</h2>
<p>The latest source code is available from the <a href="http://www.assembla.com/spaces/ldif/wiki">LDIF development page</a>
on Assembla.com.</p>
<p>The framework can be used under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache Software
License</a>.
</p>
<h2 id="history">9. Version history </h2>
<p>
</p>
<table>
  <tbody>
    <tr>
      <th> Version</th>
      <th> Release log</th>
      <th> Date</th>
    </tr>
    <tr class="even">
      <td> 0.3</td>
      <td>Access module support (dataset dump, SPARQL, crawling)<br>
Scheduler for running import and integration tasks automatically<br>
Configuration file XML schemas for validation<br>
URI minting<br>
</td>
      <td> 9/29/2011</td>
    </tr>
    <tr class="odd">
      <td> 0.2</td>
      <td> R2R data translation tasks are now executed in parallel <br>
Perform source syntax validation before loading data (optional) <br>
Support for external sameAs links <br>
RDF/N-Triples data output module <br>
Support for bzip2 source compression <br>
Improved loading perfomance <br>
Memory usage improvements: caching factum rows and string interning
only for relevant data </td>
      <td> 8/25/2011</td>
    </tr>
    <tr class="even">
      <td> 0.1</td>
      <td> Intial release of LDIF</td>
      <td> 6/29/2011</td>
    </tr>
  </tbody>
</table>
<h2 id="feedback">10. Support and Feedback </h2>
<p>For questions and feedback please use the <a href="http://groups.google.com/group/ldif?hl=en">LDIF Google Group</a>.
</p>
<h2 id="references">11. References</h2>
<ul>
</ul>
<ul>
</ul>
<ol>
  <li>Tom Heath, Christian Bizer: <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00334ED1V01Y201102WBE001">Linked
Data: Evolving the Web into a Global Data Space</a>. Synthesis
Lectures on the Semantic Web: Theory and Technology, Morgan &amp;
Claypool Publishers, ISBN <a href="http://www.amazon.com/Linked-Data-Synthesis-Lectures-Engineering/dp/1608454304/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1297846088&amp;sr=8-1">978160845431</a>,
2011 ( <a href="http://linkeddatabook.com/">Free HTML version</a> ).</li>
  <li>Christian Bizer, Andreas Schultz: <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/BizerSchultz-COLD-R2R-Paper.pdf">The
R2R Framework: Publishing and Discovering Mappings on the Web</a> ( <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/BizerSchultz-COLD-R2R-Talk.pdf">Slides</a>
). 1st International Workshop on Consuming Linked Data (COLD
2010), Shanghai, November 2010. </li>
  <li>Julius Volz, Christian Bizer, Martin Gaedke, Georgi Kobilarov: <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/VolzBizerGaedkeKobilarov-ISWC2009-Silk.pdf">Discovering
and Maintaining Links on the Web of Data</a> ( <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/VolzBizerGaedkeKobilarov-ISWC2009-Silk-Talk.pdf">Slides</a>
). International Semantic Web Conference (ISWC2009),
Westfields, USA, October 2009. </li>
  <li>Robert Isele, Anja Jentzsch, Christian Bizer: <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/IseleJentzschBizer-Silk-Cold2010.pdf">Silk
Server - Adding missing Links while consuming Linked Data</a> ( <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/IseleJentzschBizer-Silk-Cold2010-Talk.pdf">Slides</a>
). 1st International Workshop on Consuming Linked Data (COLD
2010), Shanghai, November 2010.</li>
  <li>Andreas Schultz, Andrea Matteini, Robert Isele, Christian Bizer,
Christian Becker: <a href="http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Schultz-etal-Cold2011-LDIF-Paper.pdf">LDIF
- Linked Data Integration Framework</a>. 2nd International Workshop on
Consuming Linked Data, Bonn, Germany, October 2011.<span style="border-collapse: separate; color: rgb(0, 0, 0); font-family:'Times New Roman'; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; font-size: medium;"><span style="color: rgb(51, 51, 51); font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 11px; line-height: 15px;"><br>
    </span></span></li>
</ol>
<h2 id="acknowledgments">12. Acknowledgments </h2>
<p>This work was supported in part by Vulcan Inc. as part of its <a href="http://www.projecthalo.com">Project Halo</a> and by the EU FP7
project <a href="http://lod2.eu/">LOD2 - Creating Knowledge out of
Interlinked Data</a> (Grant No. 257943). </p>
</div>

</body></html>